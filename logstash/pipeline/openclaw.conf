# Logstash Pipeline for OpenClaw File Logs
# Optional: Use this to ingest existing file logs into Elasticsearch

input {
  file {
    path => "/var/log/openclaw/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => json_lines
  }
}

filter {
  # Parse the JSON log entry
  if [msg] {
    mutate {
      rename => { "msg" => "message" }
    }
  }

  # Extract timestamp
  if [time] {
    date {
      match => ["time", "ISO8601"]
      target => "@timestamp"
    }
    mutate {
      remove_field => ["time"]
    }
  }

  # Map log levels
  if [level] {
    translate {
      field => "level"
      destination => "log_level"
      dictionary => {
        "10" => "trace"
        "20" => "debug"
        "30" => "info"
        "40" => "warn"
        "50" => "error"
        "60" => "fatal"
      }
      fallback => "unknown"
    }
  }

  # Extract subsystem from name field
  if [name] {
    mutate {
      add_field => { "subsystem" => "%{name}" }
    }
  }

  # Parse session info if present
  if [sessionKey] {
    mutate {
      add_field => { "session.key" => "%{sessionKey}" }
    }
  }

  # Add source identifier
  mutate {
    add_field => { "source" => "openclaw-gateway" }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "openclaw-filelogs-%{+YYYY.MM.dd}"
  }

  # Debug output (remove in production)
  # stdout { codec => rubydebug }
}
